[Mode]
# Set logging level
debug = True

[Glidein]
# Address where the state of the remote queue can be accessed
address = http://pyglidein-server:8001/jsonrpc
cluster = pyglidein-server

# Whether or not the state of the remote queue was queried
# from the server or whether it was transmitted through a
# a text file
ssh_state = False

# How long to wait before considering the next set of jobs
delay = -1

# Location of the tarball, etc.
loc = $HOME/glidein

# Filename of tarball to be extracted
tarball = /opt/glidein.tar.gz

# Filename of the executable
executable = glidein_start.sh

# run every 10 seconds
delay = 10
site = WIPAC_Dev

[Cluster]
# User under which the jobs are being submitted
user = pyglidein

# OS of the cluster
os = RHEL7

# Scheduler used by cluster
scheduler = HTCondor

# Submit command for scheduler
submit_command = condor_submit

# Maximum number jobs that be in the queue
max_total_jobs = 25

# Number of jobs that can be submitted per round
limit_per_submit = 5

# Is cvmfs available? True/False
cvmfs = True

# Can we submit only jobs that need CPUs? True/False
cpu_only = False

# Can we submit only jobs that need GPUs? True/False
gpu_only = False

# A list of according to which job requirement the
# job submission should be prioritized. The position
# in the list indicates the prioritization. ["memory", "disk"]
# means jobs with high memory will be
# submitted before jobs with lower memory requirements,
# followed by jobs with high disk vs. low disk requirement.
# Jobs with high memory and disk requirements will be submitted first
# then jobs with high memory and medium disk requirements, and so on
# and so forth.
prioritize_jobs = ["memory", "disk"]

# Command needed to determine the number of jobs running
running_cmd = condor_q -all -constraint 'Owner == "${USER}" && JobStatus == 2' -af ClusterID | wc -l
idle_cmd = condor_q -constraint 'Owner == "${USER}" && JobStatus == 1' -af ClusterId | wc -l

# Group jobs with the same requirements into a
# single submission.
# Special note for PBS:
# The option `group_jobs` cannot be used with PBS.
# gridftp does not like the name of the temporary
# directories generated by PBS. The name has a
# `[]` in it, which gridftp takes offense to.
# groups_jobs = True
whole_node = False
walltime_hrs = 1

[SubmitFile]
# Filename of the submit file
filename = submit.condor

# Filename of environment wrapper for HTCondor submit script
env_wrapper_name = env_wrapper.sh

[CustomEnv]
# Special enviroment variables that need to be set
http_proxy = http://squid.icecube.wisc.edu:3128

[StartdLogging]
send_startd_logs = True
url = s3.amazonaws.com
bucket = pyglidein-logging-wipac-dev

[StardChecks]
enable_startd_checks = True
